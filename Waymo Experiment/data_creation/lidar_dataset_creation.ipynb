{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppresses INFO, WARNING, and ERROR messages -> Supress tensorflow warnings\n",
    "from os import path as osp\n",
    "\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "if not tf.executing_eagerly():\n",
    "  tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "from waymo_open_dataset import dataset_pb2 as open_dataset\n",
    "from waymo_open_dataset.utils import range_image_utils, transform_utils\n",
    "from waymo_open_dataset.utils.frame_utils import parse_range_image_and_camera_projection\n",
    "\n",
    "from io import BytesIO\n",
    "import copy\n",
    "\n",
    "# import utils.utils as utils\n",
    "# from utils.lidar_box3d import LiDARInstance3DBoxes\n",
    "# from utils.box_3d_mode import Box3DMode\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed=0, deterministic = True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    if deterministic:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variation = 'night'\n",
    "type_ = 'validation'\n",
    "\n",
    "data_path = '/mnt/data/ataparia/waymo_perception_dataset_v1_4_3'\n",
    "save_path = f'/mnt/data/ataparia/LidarTraining/{variation}-frames'\n",
    "\n",
    "# Create the save directory\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "os.makedirs(f'{save_path}/training', exist_ok=True)\n",
    "os.makedirs(f'{save_path}/validation', exist_ok=True)\n",
    "os.makedirs(f'{save_path}/testing', exist_ok=True)\n",
    "\n",
    "all_sf_files = {\n",
    "    'dawn_dusk': [], \n",
    "    'day': [], \n",
    "    'night': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = os.listdir(os.path.join(data_path, 'training'))\n",
    "train_files = [f for f in train_files if f.endswith('.tfrecord')]\n",
    "print('Number of training files:', len(train_files))\n",
    "\n",
    "val_files = os.listdir(os.path.join(data_path, 'validation'))\n",
    "val_files = [f for f in val_files if f.endswith('.tfrecord')]\n",
    "print('Number of validation files:', len(val_files))\n",
    "\n",
    "for f in tqdm(train_files, desc=\"Processing files in training\"):\n",
    "    filename = os.path.join(data_path, 'training', f)\n",
    "    dataset = tf.data.TFRecordDataset(filename, compression_type='')\n",
    "    for data in dataset:\n",
    "        frame = open_dataset.Frame()\n",
    "        frame.ParseFromString(bytearray(data.numpy()))\n",
    "        frame_stats = frame.context.stats\n",
    "        if frame_stats.location == 'location_sf':\n",
    "            time_of_day = frame_stats.time_of_day\n",
    "            if time_of_day == 'Night':\n",
    "                time_of_day = 'night'\n",
    "            elif time_of_day == 'Day':\n",
    "                time_of_day = 'day'\n",
    "            elif time_of_day == 'Dawn/Dusk':\n",
    "                time_of_day = 'dawn_dusk'   \n",
    "            all_sf_files[time_of_day].append(filename)\n",
    "        break\n",
    "\n",
    "# Validation split\n",
    "for f in tqdm(val_files, desc=\"Processing files in validation\"):\n",
    "    filename = os.path.join(data_path, 'validation', f)\n",
    "    dataset = tf.data.TFRecordDataset(filename, compression_type='')\n",
    "    for data in dataset:\n",
    "        frame = open_dataset.Frame()\n",
    "        frame.ParseFromString(bytearray(data.numpy()))\n",
    "        frame_stats = frame.context.stats\n",
    "        if frame_stats.location == 'location_sf':\n",
    "            time_of_day = frame_stats.time_of_day\n",
    "            if time_of_day == 'Night':\n",
    "                time_of_day = 'night'\n",
    "            elif time_of_day == 'Day':\n",
    "                time_of_day = 'day'\n",
    "            elif time_of_day == 'Dawn/Dusk':\n",
    "                time_of_day = 'dawn_dusk'            \n",
    "            all_sf_files[time_of_day].append(filename)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_to_frames = []\n",
    "\n",
    "files = all_sf_files[variation]\n",
    "\n",
    "for f in tqdm(files, desc=\"Processing files\"):\n",
    "    filename = f\n",
    "    dataset = tf.data.TFRecordDataset(filename, compression_type='')\n",
    "    for data in dataset:\n",
    "        frame = open_dataset.Frame()\n",
    "        frame.ParseFromString(bytearray(data.numpy()))\n",
    "        \n",
    "        video_to_frames.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(video_to_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of {variation} files:', len(video_to_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = video_to_frames[:int(0.8*len(video_to_frames))]\n",
    "val_files = video_to_frames[int(0.8*len(video_to_frames)):int(0.9*len(video_to_frames))]\n",
    "test_files = video_to_frames[int(0.9*len(video_to_frames)):]\n",
    "\n",
    "print('Number of train files:', len(train_files))\n",
    "print('Number of val files:', len(val_files))\n",
    "print('Number of test files:', len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_LENGTH = 7\n",
    "MAX_SWEEPS = 1\n",
    "\n",
    "# keep the order defined by the official protocol\n",
    "CAM_LIST = [\n",
    "    '_FRONT',\n",
    "    '_FRONT_LEFT',\n",
    "    '_FRONT_RIGHT',\n",
    "    '_SIDE_LEFT',\n",
    "    '_SIDE_RIGHT',\n",
    "]\n",
    "LIDAR_LIST = ['TOP', 'FRONT', 'SIDE_LEFT', 'SIDE_RIGHT', 'REAR']\n",
    "TYPE_LIST = [\n",
    "    'UNKNOWN', 'VEHICLE', 'PEDESTRIAN', 'SIGN', 'CYCLIST'\n",
    "]\n",
    "\n",
    "# MMDetection3D unified camera keys & class names\n",
    "CAMERA_TYPES = [\n",
    "    'CAM_FRONT',\n",
    "    'CAM_FRONT_LEFT',\n",
    "    'CAM_FRONT_RIGHT',\n",
    "    'CAM_SIDE_LEFT',\n",
    "    'CAM_SIDE_RIGHT',\n",
    "]\n",
    "SELECTED_WAYMO_CLASSES = ['VEHICLE', 'PEDESTRIAN', 'CYCLIST']\n",
    "INFO_MAP = {\n",
    "    'training': '_infos_train.pkl',\n",
    "    'validation': '_infos_val.pkl',\n",
    "    'testing': '_infos_test.pkl'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data folders\n",
    "image_save_path = os.path.join(save_path, type_, 'image_')\n",
    "pointcloud_save_path = os.path.join(save_path, type_, 'velodyne')\n",
    "\n",
    "os.makedirs(pointcloud_save_path, exist_ok=True)\n",
    "for i in range(5):\n",
    "    os.makedirs(f\"{image_save_path}{str(i)}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(frame, frame_idx, image_save_dir):\n",
    "    \"\"\"Parse and save the images in jpg format.\n",
    "\n",
    "    Args:\n",
    "        frame (:obj:`Frame`): Open dataset frame proto.\n",
    "        frame_idx (int): Current frame index.\n",
    "    \"\"\"\n",
    "    for img in frame.images:\n",
    "        img_path = f'{image_save_dir}{str(img.name - 1)}/' + \\\n",
    "            f'{str(frame_idx).zfill(INDEX_LENGTH)}.jpg'\n",
    "        with open(img_path, 'wb') as fp:\n",
    "            fp.write(img.image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_lidar(frame, point_cloud_save_dir, frame_idx):\n",
    "    \n",
    "    def convert_range_image_to_point_cloud(frame,\n",
    "                                           range_images,\n",
    "                                           camera_projections,\n",
    "                                           range_image_top_pose,\n",
    "                                           ri_index=0,\n",
    "                                           filter_no_label_zone_points=True):\n",
    "        \"\"\"Convert range images to point cloud.\n",
    "\n",
    "        Args:\n",
    "            frame (:obj:`Frame`): Open dataset frame.\n",
    "            range_images (dict): Mapping from laser_name to list of two\n",
    "                range images corresponding with two returns.\n",
    "            camera_projections (dict): Mapping from laser_name to list of two\n",
    "                camera projections corresponding with two returns.\n",
    "            range_image_top_pose (:obj:`Transform`): Range image pixel pose for\n",
    "                top lidar.\n",
    "            ri_index (int, optional): 0 for the first return,\n",
    "                1 for the second return. Default: 0.\n",
    "\n",
    "        Returns:\n",
    "            tuple[list[np.ndarray]]: (List of points with shape [N, 3],\n",
    "                camera projections of points with shape [N, 6], intensity\n",
    "                with shape [N, 1], elongation with shape [N, 1], points'\n",
    "                position in the depth map (element offset if points come from\n",
    "                the main lidar otherwise -1) with shape[N, 1]). All the\n",
    "                lists have the length of lidar numbers (5).\n",
    "        \"\"\"\n",
    "        calibrations = sorted(\n",
    "            frame.context.laser_calibrations, key=lambda c: c.name)\n",
    "        points = []\n",
    "        cp_points = []\n",
    "        intensity = []\n",
    "        elongation = []\n",
    "        mask_indices = []\n",
    "\n",
    "        frame_pose = tf.convert_to_tensor(\n",
    "            value=np.reshape(np.array(frame.pose.transform), [4, 4]))\n",
    "        # [H, W, 6]\n",
    "        range_image_top_pose_tensor = tf.reshape(\n",
    "            tf.convert_to_tensor(value=range_image_top_pose.data),\n",
    "            range_image_top_pose.shape.dims)\n",
    "        # [H, W, 3, 3]\n",
    "        range_image_top_pose_tensor_rotation = \\\n",
    "            transform_utils.get_rotation_matrix(\n",
    "                range_image_top_pose_tensor[..., 0],\n",
    "                range_image_top_pose_tensor[..., 1],\n",
    "                range_image_top_pose_tensor[..., 2])\n",
    "        range_image_top_pose_tensor_translation = \\\n",
    "            range_image_top_pose_tensor[..., 3:]\n",
    "        range_image_top_pose_tensor = transform_utils.get_transform(\n",
    "            range_image_top_pose_tensor_rotation,\n",
    "            range_image_top_pose_tensor_translation)\n",
    "        for c in calibrations:\n",
    "            range_image = range_images[c.name][ri_index]\n",
    "            if len(c.beam_inclinations) == 0:\n",
    "                beam_inclinations = range_image_utils.compute_inclination(\n",
    "                    tf.constant(\n",
    "                        [c.beam_inclination_min, c.beam_inclination_max]),\n",
    "                    height=range_image.shape.dims[0])\n",
    "            else:\n",
    "                beam_inclinations = tf.constant(c.beam_inclinations)\n",
    "\n",
    "            beam_inclinations = tf.reverse(beam_inclinations, axis=[-1])\n",
    "            extrinsic = np.reshape(np.array(c.extrinsic.transform), [4, 4])\n",
    "\n",
    "            range_image_tensor = tf.reshape(\n",
    "                tf.convert_to_tensor(value=range_image.data),\n",
    "                range_image.shape.dims)\n",
    "            pixel_pose_local = None\n",
    "            frame_pose_local = None\n",
    "            if c.name == open_dataset.LaserName.TOP:\n",
    "                pixel_pose_local = range_image_top_pose_tensor\n",
    "                pixel_pose_local = tf.expand_dims(pixel_pose_local, axis=0)\n",
    "                frame_pose_local = tf.expand_dims(frame_pose, axis=0)\n",
    "            range_image_mask = range_image_tensor[..., 0] > 0\n",
    "\n",
    "            if filter_no_label_zone_points:\n",
    "                nlz_mask = range_image_tensor[..., 3] != 1.0  # 1.0: in NLZ\n",
    "                range_image_mask = range_image_mask & nlz_mask\n",
    "\n",
    "            range_image_cartesian = \\\n",
    "                range_image_utils.extract_point_cloud_from_range_image(\n",
    "                    tf.expand_dims(range_image_tensor[..., 0], axis=0),\n",
    "                    tf.expand_dims(extrinsic, axis=0),\n",
    "                    tf.expand_dims(tf.convert_to_tensor(\n",
    "                        value=beam_inclinations), axis=0),\n",
    "                    pixel_pose=pixel_pose_local,\n",
    "                    frame_pose=frame_pose_local)\n",
    "\n",
    "            mask_index = tf.where(range_image_mask)\n",
    "\n",
    "            range_image_cartesian = tf.squeeze(range_image_cartesian, axis=0)\n",
    "            points_tensor = tf.gather_nd(range_image_cartesian, mask_index)\n",
    "\n",
    "            cp = camera_projections[c.name][ri_index]\n",
    "            cp_tensor = tf.reshape(\n",
    "                tf.convert_to_tensor(value=cp.data), cp.shape.dims)\n",
    "            cp_points_tensor = tf.gather_nd(cp_tensor, mask_index)\n",
    "            points.append(points_tensor.numpy())\n",
    "            cp_points.append(cp_points_tensor.numpy())\n",
    "\n",
    "            intensity_tensor = tf.gather_nd(range_image_tensor[..., 1],\n",
    "                                            mask_index)\n",
    "            intensity.append(intensity_tensor.numpy())\n",
    "\n",
    "            elongation_tensor = tf.gather_nd(range_image_tensor[..., 2],\n",
    "                                             mask_index)\n",
    "            elongation.append(elongation_tensor.numpy())\n",
    "            if c.name == 1:\n",
    "                mask_index = (ri_index * range_image_mask.shape[0] +\n",
    "                              mask_index[:, 0]\n",
    "                              ) * range_image_mask.shape[1] + mask_index[:, 1]\n",
    "                mask_index = mask_index.numpy().astype(elongation[-1].dtype)\n",
    "            else:\n",
    "                mask_index = np.full_like(elongation[-1], -1)\n",
    "\n",
    "            mask_indices.append(mask_index)\n",
    "\n",
    "        return points, cp_points, intensity, elongation, mask_indices\n",
    "    \n",
    "    \"\"\"Parse and save the lidar data in psd format.\n",
    "\n",
    "    Args:\n",
    "        frame (:obj:`Frame`): Open dataset frame proto.\n",
    "        file_idx (int): Current file index.\n",
    "        frame_idx (int): Current frame index.\n",
    "    \"\"\"\n",
    "    range_images, camera_projections, seg_labels, range_image_top_pose = \\\n",
    "        parse_range_image_and_camera_projection(frame)\n",
    "\n",
    "    if range_image_top_pose is None:\n",
    "        # the camera only split doesn't contain lidar points.\n",
    "        return\n",
    "    # First return\n",
    "    points_0, cp_points_0, intensity_0, elongation_0, mask_indices_0 = \\\n",
    "        convert_range_image_to_point_cloud(\n",
    "            frame,\n",
    "            range_images,\n",
    "            camera_projections,\n",
    "            range_image_top_pose,\n",
    "            ri_index=0\n",
    "        )\n",
    "    points_0 = np.concatenate(points_0, axis=0)\n",
    "    intensity_0 = np.concatenate(intensity_0, axis=0)\n",
    "    elongation_0 = np.concatenate(elongation_0, axis=0)\n",
    "    mask_indices_0 = np.concatenate(mask_indices_0, axis=0)\n",
    "\n",
    "    # Second return\n",
    "    points_1, cp_points_1, intensity_1, elongation_1, mask_indices_1 = \\\n",
    "        convert_range_image_to_point_cloud(\n",
    "            frame,\n",
    "            range_images,\n",
    "            camera_projections,\n",
    "            range_image_top_pose,\n",
    "            ri_index=1\n",
    "        )\n",
    "    points_1 = np.concatenate(points_1, axis=0)\n",
    "    intensity_1 = np.concatenate(intensity_1, axis=0)\n",
    "    elongation_1 = np.concatenate(elongation_1, axis=0)\n",
    "    mask_indices_1 = np.concatenate(mask_indices_1, axis=0)\n",
    "\n",
    "    points = np.concatenate([points_0, points_1], axis=0)\n",
    "    intensity = np.concatenate([intensity_0, intensity_1], axis=0)\n",
    "    elongation = np.concatenate([elongation_0, elongation_1], axis=0)\n",
    "    mask_indices = np.concatenate([mask_indices_0, mask_indices_1], axis=0)\n",
    "\n",
    "    # timestamp = frame.timestamp_micros * np.ones_like(intensity)\n",
    "\n",
    "    # concatenate x,y,z, intensity, elongation, timestamp (6-dim)\n",
    "    point_cloud = np.column_stack(\n",
    "        (points, intensity, elongation, mask_indices))\n",
    "\n",
    "    pc_path = f'{point_cloud_save_dir}/' + \\\n",
    "        f'{str(frame_idx).zfill(INDEX_LENGTH)}.bin'\n",
    "    point_cloud.astype(np.float32).tofile(pc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cart_to_homo(self, mat):\n",
    "    \"\"\"Convert transformation matrix in Cartesian coordinates to\n",
    "    homogeneous format.\n",
    "\n",
    "    Args:\n",
    "        mat (np.ndarray): Transformation matrix in Cartesian.\n",
    "            The input matrix shape is 3x3 or 3x4.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Transformation matrix in homogeneous format.\n",
    "            The matrix shape is 4x4.\n",
    "    \"\"\"\n",
    "    ret = np.eye(4)\n",
    "    if mat.shape == (3, 3):\n",
    "        ret[:3, :3] = mat\n",
    "    elif mat.shape == (3, 4):\n",
    "        ret[:3, :] = mat\n",
    "    else:\n",
    "        raise ValueError(mat.shape)\n",
    "    return ret\n",
    "\n",
    "def gather_instance_info(frame, cam_sync=False):\n",
    "    \"\"\"Generate instances and cam_sync_instances infos.\n",
    "\n",
    "    For more details about infos, please refer to:\n",
    "    https://mmdetection3d.readthedocs.io/en/latest/advanced_guides/datasets/waymo.html\n",
    "    \"\"\"  # noqa: E501\n",
    "    filter_empty_3dboxes = True\n",
    "    id_to_bbox = dict()\n",
    "    id_to_name = dict()\n",
    "    for labels in frame.projected_lidar_labels:\n",
    "        name = labels.name\n",
    "        for label in labels.labels:\n",
    "            # TODO: need a workaround as bbox may not belong to front cam\n",
    "            bbox = [\n",
    "                label.box.center_x - label.box.length / 2,\n",
    "                label.box.center_y - label.box.width / 2,\n",
    "                label.box.center_x + label.box.length / 2,\n",
    "                label.box.center_y + label.box.width / 2\n",
    "            ]\n",
    "            id_to_bbox[label.id] = bbox\n",
    "            id_to_name[label.id] = name - 1\n",
    "\n",
    "    group_id = 0\n",
    "    instance_infos = []\n",
    "    for obj in frame.laser_labels:\n",
    "        instance_info = dict()\n",
    "        bounding_box = None\n",
    "        name = None\n",
    "        id = obj.id\n",
    "        for proj_cam in CAM_LIST:\n",
    "            if id + proj_cam in id_to_bbox:\n",
    "                bounding_box = id_to_bbox.get(id + proj_cam)\n",
    "                name = id_to_name.get(id + proj_cam)\n",
    "                break\n",
    "\n",
    "        # NOTE: the 2D labels do not have strict correspondence with\n",
    "        # the projected 2D lidar labels\n",
    "        # e.g.: the projected 2D labels can be in camera 2\n",
    "        # while the most_visible_camera can have id 4\n",
    "        if cam_sync:\n",
    "            if obj.most_visible_camera_name:\n",
    "                name = CAM_LIST.index(\n",
    "                    f'_{obj.most_visible_camera_name}')\n",
    "                box3d = obj.camera_synced_box\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            box3d = obj.box\n",
    "\n",
    "        if bounding_box is None or name is None:\n",
    "            name = 0\n",
    "            bounding_box = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "        my_type = TYPE_LIST[obj.type]\n",
    "\n",
    "        if my_type not in SELECTED_WAYMO_CLASSES:\n",
    "            continue\n",
    "        else:\n",
    "            label = SELECTED_WAYMO_CLASSES.index(my_type)\n",
    "\n",
    "        if filter_empty_3dboxes and obj.num_lidar_points_in_box < 1:\n",
    "            continue\n",
    "\n",
    "        group_id += 1\n",
    "        instance_info['group_id'] = group_id\n",
    "        instance_info['camera_id'] = name\n",
    "        instance_info['bbox'] = bounding_box\n",
    "        instance_info['bbox_label'] = label\n",
    "\n",
    "        height = box3d.height\n",
    "        width = box3d.width\n",
    "        length = box3d.length\n",
    "\n",
    "        # NOTE: We save the bottom center of 3D bboxes.\n",
    "        x = box3d.center_x\n",
    "        y = box3d.center_y\n",
    "        z = box3d.center_z - height / 2\n",
    "\n",
    "        rotation_y = box3d.heading\n",
    "\n",
    "        instance_info['bbox_3d'] = np.array(\n",
    "            [x, y, z, length, width, height,\n",
    "                rotation_y]).astype(np.float32).tolist()\n",
    "        instance_info['bbox_label_3d'] = label\n",
    "        instance_info['num_lidar_pts'] = obj.num_lidar_points_in_box\n",
    "\n",
    "        # if self.save_track_id:\n",
    "        #     instance_info['track_id'] = obj.id\n",
    "        instance_infos.append(instance_info)\n",
    "    return instance_infos\n",
    "\n",
    "def gather_cam_instance_info(self, instances: dict, images: dict):\n",
    "    \"\"\"Generate cam_instances infos.\n",
    "\n",
    "    For more details about infos, please refer to:\n",
    "    https://mmdetection3d.readthedocs.io/en/latest/advanced_guides/datasets/waymo.html\n",
    "    \"\"\"  # noqa: E501\n",
    "    cam_instances = dict()\n",
    "    for cam_type in self.camera_types:\n",
    "        lidar2cam = np.array(images[cam_type]['lidar2cam'])\n",
    "        cam2img = np.array(images[cam_type]['cam2img'])\n",
    "        cam_instances[cam_type] = []\n",
    "        for instance in instances:\n",
    "            cam_instance = dict()\n",
    "            gt_bboxes_3d = np.array(instance['bbox_3d'])\n",
    "            # Convert lidar coordinates to camera coordinates\n",
    "            gt_bboxes_3d = LiDARInstance3DBoxes(\n",
    "                gt_bboxes_3d[None, :]).convert_to(\n",
    "                    Box3DMode.CAM, lidar2cam, correct_yaw=True)\n",
    "            corners_3d = gt_bboxes_3d.corners.numpy()\n",
    "            corners_3d = corners_3d[0].T  # (1, 8, 3) -> (3, 8)\n",
    "            in_camera = np.argwhere(corners_3d[2, :] > 0).flatten()\n",
    "            corners_3d = corners_3d[:, in_camera]\n",
    "            # Project 3d box to 2d.\n",
    "            corner_coords = utils.view_points(corners_3d, cam2img,\n",
    "                                        True).T[:, :2].tolist()\n",
    "\n",
    "            # Keep only corners that fall within the image.\n",
    "            # TODO: imsize should be determined by the current image size\n",
    "            # CAM_FRONT: (1920, 1280)\n",
    "            # CAM_FRONT_LEFT: (1920, 1280)\n",
    "            # CAM_SIDE_LEFT: (1920, 886)\n",
    "            final_coords = utils.post_process_coords(\n",
    "                corner_coords,\n",
    "                imsize=(images['CAM_FRONT']['width'],\n",
    "                        images['CAM_FRONT']['height']))\n",
    "\n",
    "            # Skip if the convex hull of the re-projected corners\n",
    "            # does not intersect the image canvas.\n",
    "            if final_coords is None:\n",
    "                continue\n",
    "            else:\n",
    "                min_x, min_y, max_x, max_y = final_coords\n",
    "\n",
    "            cam_instance['bbox'] = [min_x, min_y, max_x, max_y]\n",
    "            cam_instance['bbox_label'] = instance['bbox_label']\n",
    "            cam_instance['bbox_3d'] = gt_bboxes_3d.numpy().squeeze(\n",
    "            ).astype(np.float32).tolist()\n",
    "            cam_instance['bbox_label_3d'] = instance['bbox_label_3d']\n",
    "\n",
    "            center_3d = gt_bboxes_3d.gravity_center.numpy()\n",
    "            center_2d_with_depth = utils.points_cam2img(\n",
    "                center_3d, cam2img, with_depth=True)\n",
    "            center_2d_with_depth = center_2d_with_depth.squeeze().tolist()\n",
    "\n",
    "            # normalized center2D + depth\n",
    "            # if samples with depth < 0 will be removed\n",
    "            if center_2d_with_depth[2] <= 0:\n",
    "                continue\n",
    "            cam_instance['center_2d'] = center_2d_with_depth[:2]\n",
    "            cam_instance['depth'] = center_2d_with_depth[2]\n",
    "\n",
    "            # TODO: Discuss whether following info is necessary\n",
    "            cam_instance['bbox_3d_isvalid'] = True\n",
    "            cam_instance['velocity'] = -1\n",
    "            cam_instances[cam_type].append(cam_instance)\n",
    "\n",
    "    return cam_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_waymo_info_file(frame, frame_idx, file_infos, test_mode, save_cam_sync_instances, save_cam_instances):\n",
    "    r\"\"\"Generate waymo train/val/test infos.\n",
    "\n",
    "    For more details about infos, please refer to:\n",
    "    https://mmdetection3d.readthedocs.io/en/latest/advanced_guides/datasets/waymo.html\n",
    "    \"\"\"  # noqa: E501\n",
    "    frame_infos = dict()\n",
    "\n",
    "    # Gather frame infos\n",
    "    sample_idx = f'{str(frame_idx).zfill(INDEX_LENGTH)}'\n",
    "    frame_infos['sample_idx'] = int(sample_idx)\n",
    "    frame_infos['timestamp'] = frame.timestamp_micros\n",
    "    frame_infos['ego2global'] = np.array(frame.pose.transform).reshape(\n",
    "        4, 4).astype(np.float32).tolist()\n",
    "    frame_infos['context_name'] = frame.context.name\n",
    "    frame_infos['stats'] = {\n",
    "        'location': frame.context.stats.location,\n",
    "        'time_of_day': frame.context.stats.time_of_day,\n",
    "        'weather': frame.context.stats.weather,\n",
    "    }\n",
    "\n",
    "    # Gather camera infos\n",
    "    frame_infos['images'] = dict()\n",
    "    # waymo front camera to kitti reference camera\n",
    "    T_front_cam_to_ref = np.array([[0.0, -1.0, 0.0], [0.0, 0.0, -1.0],\n",
    "                                    [1.0, 0.0, 0.0]])\n",
    "    camera_calibs = []\n",
    "    Tr_velo_to_cams = []\n",
    "    for camera in frame.context.camera_calibrations:\n",
    "        # extrinsic parameters\n",
    "        T_cam_to_vehicle = np.array(camera.extrinsic.transform).reshape(\n",
    "            4, 4)\n",
    "        T_vehicle_to_cam = np.linalg.inv(T_cam_to_vehicle)\n",
    "        Tr_velo_to_cam = \\\n",
    "            cart_to_homo(T_front_cam_to_ref) @ T_vehicle_to_cam\n",
    "        Tr_velo_to_cams.append(Tr_velo_to_cam)\n",
    "\n",
    "        # intrinsic parameters\n",
    "        camera_calib = np.zeros((3, 4))\n",
    "        camera_calib[0, 0] = camera.intrinsic[0]\n",
    "        camera_calib[1, 1] = camera.intrinsic[1]\n",
    "        camera_calib[0, 2] = camera.intrinsic[2]\n",
    "        camera_calib[1, 2] = camera.intrinsic[3]\n",
    "        camera_calib[2, 2] = 1\n",
    "        camera_calibs.append(camera_calib)\n",
    "\n",
    "    for i, (cam_key, camera_calib, Tr_velo_to_cam) in enumerate(zip(CAMERA_TYPES, camera_calibs, Tr_velo_to_cams)):\n",
    "        cam_infos = dict()\n",
    "        cam_infos['img_path'] = str(sample_idx) + '.jpg'\n",
    "        # NOTE: frames.images order is different\n",
    "        for img in frame.images:\n",
    "            if img.name == i + 1:\n",
    "                width, height = Image.open(BytesIO(img.image)).size\n",
    "        cam_infos['height'] = height\n",
    "        cam_infos['width'] = width\n",
    "        cam_infos['lidar2cam'] = Tr_velo_to_cam.astype(np.float32).tolist()\n",
    "        cam_infos['cam2img'] = camera_calib.astype(np.float32).tolist()\n",
    "        cam_infos['lidar2img'] = (camera_calib @ Tr_velo_to_cam).astype(\n",
    "            np.float32).tolist()\n",
    "        frame_infos['images'][cam_key] = cam_infos\n",
    "\n",
    "    # Gather lidar infos\n",
    "    lidar_infos = dict()\n",
    "    lidar_infos['lidar_path'] = str(sample_idx) + '.bin'\n",
    "    lidar_infos['num_pts_feats'] = 6\n",
    "    frame_infos['lidar_points'] = lidar_infos\n",
    "\n",
    "    # Gather lidar sweeps and camera sweeps infos\n",
    "    # TODO: Add lidar2img in image sweeps infos when we need it.\n",
    "    # TODO: Consider merging lidar sweeps infos and image sweeps infos.\n",
    "    lidar_sweeps_infos, image_sweeps_infos = [], []\n",
    "    for prev_offset in range(-1, -MAX_SWEEPS - 1, -1):\n",
    "        prev_lidar_infos = dict()\n",
    "        prev_image_infos = dict()\n",
    "        if frame_idx + prev_offset >= 0:\n",
    "            prev_frame_infos = file_infos[prev_offset]\n",
    "            prev_lidar_infos['timestamp'] = prev_frame_infos['timestamp']\n",
    "            prev_lidar_infos['ego2global'] = prev_frame_infos['ego2global']\n",
    "            prev_lidar_infos['lidar_points'] = dict()\n",
    "            lidar_path = prev_frame_infos['lidar_points']['lidar_path']\n",
    "            prev_lidar_infos['lidar_points']['lidar_path'] = lidar_path\n",
    "            lidar_sweeps_infos.append(prev_lidar_infos)\n",
    "\n",
    "            prev_image_infos['timestamp'] = prev_frame_infos['timestamp']\n",
    "            prev_image_infos['ego2global'] = prev_frame_infos['ego2global']\n",
    "            prev_image_infos['images'] = dict()\n",
    "            for cam_key in CAMERA_TYPES:\n",
    "                prev_image_infos['images'][cam_key] = dict()\n",
    "                img_path = prev_frame_infos['images'][cam_key]['img_path']\n",
    "                prev_image_infos['images'][cam_key]['img_path'] = img_path\n",
    "            image_sweeps_infos.append(prev_image_infos)\n",
    "    if lidar_sweeps_infos:\n",
    "        frame_infos['lidar_sweeps'] = lidar_sweeps_infos\n",
    "    if image_sweeps_infos:\n",
    "        frame_infos['image_sweeps'] = image_sweeps_infos\n",
    "\n",
    "    if not test_mode:\n",
    "        # Gather instances infos which is used for lidar-based 3D detection\n",
    "        frame_infos['instances'] = gather_instance_info(frame)\n",
    "        # Gather cam_sync_instances infos which is used for image-based\n",
    "        # (multi-view) 3D detection.\n",
    "        if save_cam_sync_instances:\n",
    "            frame_infos['cam_sync_instances'] = gather_instance_info(\n",
    "                frame, cam_sync=True)\n",
    "        # Gather cam_instances infos which is used for image-based\n",
    "        # (monocular) 3D detection (optional).\n",
    "        # TODO: Should we use cam_sync_instances to generate cam_instances?\n",
    "        if save_cam_instances:\n",
    "            frame_infos['cam_instances'] = gather_cam_instance_info(\n",
    "                copy.deepcopy(frame_infos['instances']),\n",
    "                frame_infos['images'])\n",
    "    file_infos.append(frame_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_one_frame(args, test_mode=False, save_cam_sync_instances=True, save_cam_instances=True):\n",
    "    \"\"\"\n",
    "    Process one frame: save the images and lidar point cloud,\n",
    "    create and return the info dictionary.\n",
    "    \n",
    "    Args:\n",
    "        args (tuple): A tuple containing (frame, frame_idx)\n",
    "    \n",
    "    Returns:\n",
    "        dict: the frame information (from create_waymo_info_file)\n",
    "    \"\"\"\n",
    "    \n",
    "    frame, frame_idx = args\n",
    "    \n",
    "    save_image(frame, frame_idx, image_save_path)\n",
    "    save_lidar(frame, pointcloud_save_path, frame_idx)\n",
    "    \n",
    "    local_file_infos = []\n",
    "    create_waymo_info_file(frame, frame_idx, local_file_infos, test_mode=test_mode,\n",
    "                             save_cam_sync_instances=save_cam_sync_instances, save_cam_instances=save_cam_instances)\n",
    "    \n",
    "    if local_file_infos:\n",
    "        return local_file_infos[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8\n",
    "file_infos = []\n",
    "\n",
    "files = train_files\n",
    "\n",
    "if type_ == 'testing':\n",
    "    files = test_files\n",
    "elif type_ == 'validation':\n",
    "    files = val_files\n",
    "\n",
    "args_list = list(zip(files, range(len(files))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "    for result in tqdm(executor.map(convert_one_frame, args_list), total=len(args_list),\n",
    "                           desc=\"Processing frames in parallel\"):\n",
    "            if result is not None:\n",
    "                file_infos.append(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waymo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
